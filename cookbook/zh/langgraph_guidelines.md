---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# LangGraph é›†æˆæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•åœ¨ AgentScope Runtime ä¸­é›†æˆå’Œä½¿ç”¨ LangGraph æ¥æ„å»ºå¤æ‚çš„æ™ºèƒ½ä½“å·¥ä½œæµã€‚
æ¨èä½¿ç”¨ python 3.11 æˆ–æ›´é«˜ç‰ˆæœ¬ä»¥æ”¯æŒæµæ¨¡å¼ï¼Œè¯¦æƒ…è¯·å‚è€ƒ [LangGraph](https://docs.langchain.com/oss/python/langgraph/streaming#llm-tokens)ã€‚

## ğŸ“¦ ç¤ºä¾‹è¯´æ˜

### 1. åŸºç¡€ LLM äº¤äº’

ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œæ¼”ç¤ºåœ¨ AgentScope Runtime ä¸­ä½¿ç”¨ LangGraph è¿›è¡ŒåŸºç¡€çš„ LLM äº¤äº’ï¼š

- ä½¿ç”¨æ¥è‡ª DashScope çš„ Qwen-Plus æ¨¡å‹
- å®ç°äº†ä¸€ä¸ªåŒ…å«å•ä¸ªèŠ‚ç‚¹çš„åŸºç¡€çŠ¶æ€å›¾å·¥ä½œæµ
- å±•ç¤ºäº†å¦‚ä½•ä» LLM æµå¼ä¼ è¾“å“åº”
- åŒ…å«å¯¹è¯å†å²çš„è®°å¿†ç®¡ç†
- æ¼”ç¤ºäº†ä½¿ç”¨ `StateGraph` ä¸ `START` å’Œ `call_model` èŠ‚ç‚¹

ä»¥ä¸‹æ˜¯æ ¸å¿ƒä»£ç ï¼š

```{code-cell}
# -*- coding: utf-8 -*-

import os
import uuid

from langchain.agents import AgentState, create_agent
from langchain.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.base import BaseCheckpointSaver
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.base import BaseStore
from langgraph.store.memory import InMemoryStore

from agentscope_runtime.engine import AgentApp
from agentscope_runtime.engine.schemas.agent_schemas import AgentRequest

global_short_term_memory: BaseCheckpointSaver = None
global_long_term_memory: BaseStore = None


@tool
def get_weather(location: str, date: str) -> str:
    """Get the weather for a location and date."""
    print(f"Getting weather for {location} on {date}...")
    return f"The weather in {location} is sunny with a temperature of 25Â°C."


# Create the AgentApp instance
agent_app = AgentApp(
    app_name="LangGraphAgent",
    app_description="A LangGraph-based research assistant",
)


class CustomAgentState(AgentState):
    user_id: str
    session_id: dict


# Initialize services as instance variables
@agent_app.init
async def init_func(self):
    global global_short_term_memory
    global global_long_term_memory
    self.short_term_mem = InMemorySaver()
    self.long_term_mem = InMemoryStore()
    global_short_term_memory = self.short_term_mem
    global_long_term_memory = self.long_term_mem


# Shutdown services, in this case,
# we don't use any resources, so we don't need to do anything here
@agent_app.shutdown
async def shutdown_func(self):
    pass


@agent_app.query(framework="langgraph")
async def query_func(
    self,
    msgs,
    request: AgentRequest = None,
    **kwargs,
):
    # Extract session information
    session_id = request.session_id
    user_id = request.user_id
    print(f"Received query from user {user_id} with session {session_id}")
    tools = [get_weather]
    # Choose the LLM that will drive the agent
    llm = ChatOpenAI(
        model="qwen-plus",
        api_key=os.environ.get("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    namespace_for_long_term_memory = (user_id, "memories")

    prompt = """You are a proactive research assistant. """

    agent = create_agent(
        llm,
        tools,
        system_prompt=prompt,
        checkpointer=self.short_term_mem,
        store=self.long_term_mem,
        state_schema=CustomAgentState,
        name="LangGraphAgent",
    )
    async for chunk, meta_data in agent.astream(
        input={"messages": msgs, "session_id": session_id, "user_id": user_id},
        stream_mode="messages",
        config={"configurable": {"thread_id": session_id}},
    ):
        is_last_chunk = (
            True if getattr(chunk, "chunk_position", "") == "last" else False
        )
        if meta_data["langgraph_node"] == "tools":
            memory_id = str(uuid.uuid4())
            memory = {"lastest_tool_call": chunk.name}
            global_long_term_memory.put(
                namespace_for_long_term_memory,
                memory_id,
                memory,
            )
        yield chunk, is_last_chunk


@agent_app.endpoint("/api/memory/short-term/{session_id}", methods=["GET"])
async def get_short_term_memory(session_id: str):
    if global_short_term_memory is None:
        return {"error": "Short-term memory not initialized yet."}

    config = {"configurable": {"thread_id": session_id}}

    value = await global_short_term_memory.aget_tuple(config)

    if value is None:
        return {"error": "No memory found for session_id"}

    return {
        "session_id": session_id,
        "messages": value.checkpoint["channel_values"]["messages"],
        "metadata": value.metadata,
    }


@agent_app.endpoint("/api/memory/short-term", methods=["GET"])
async def list_short_term_memory():
    if global_short_term_memory is None:
        return {"error": "Short-term memory not initialized yet."}

    result = []
    short_mems = list(global_short_term_memory.list(None))
    for short_mem in short_mems:
        ch_vals = short_mem.checkpoint["channel_values"]
        # Ignore the __pregel_tasks field, which is not serializable
        safe_dict = {
            key: value
            for key, value in ch_vals.items()
            if key != "__pregel_tasks"
        }
        result.append(safe_dict)
    return result


@agent_app.endpoint("/api/memory/long-term/{user_id}", methods=["GET"])
async def get_long_term_memory(user_id: str):
    if global_short_term_memory is None:
        return {"error": "Short-term memory not initialized yet."}
    namespace_for_long_term_memory = (user_id, "memories")
    long_term_mem = global_long_term_memory.search(
        namespace_for_long_term_memory,
    )

    def serialize_search_item(item):
        return {
            "namespace": item.namespace,
            "key": item.key,
            "value": item.value,
            "created_at": item.created_at,
            "updated_at": item.updated_at,
            "score": item.score,
        }

    serialized = [serialize_search_item(item) for item in long_term_mem]
    return serialized


if __name__ == "__main__":
    agent_app.run(host="127.0.0.1", port=8090)
```

### 2. å…·æœ‰å·¥å…·è°ƒç”¨èƒ½åŠ›çš„é«˜çº§æ™ºèƒ½ä½“

ä¸€ä¸ªæ›´å¤æ‚çš„ç¤ºä¾‹ï¼Œæ¼”ç¤ºå…·æœ‰å·¥å…·è°ƒç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼š

- å®ç°äº†çŸ­æœŸï¼ˆå¯¹è¯ï¼‰å’Œé•¿æœŸï¼ˆæŒä¹…åŒ–ï¼‰è®°å¿†
- ä½¿ç”¨æ£€æŸ¥ç‚¹æœºåˆ¶è·¨ä¼šè¯ä¿æŒçŠ¶æ€
- æä¾›äº†ç”¨äºæ£€æŸ¥å†…å­˜çŠ¶æ€çš„ API ç«¯ç‚¹
- åŒ…å«ä¸€ä¸ªç”¨äºæ¼”ç¤ºç›®çš„çš„è‡ªå®šä¹‰å¤©æ°”å·¥å…·
- ä½¿ç”¨å¸¦æœ‰ user_id å’Œ session_id å­—æ®µçš„è‡ªå®šä¹‰ `AgentState` æ‰©å±•
- å®ç°äº†å·¥å…·è°ƒç”¨ç»“æœçš„é•¿æœŸè®°å¿†å­˜å‚¨

ä»¥ä¸‹æ ¸å¿ƒä»£ç ï¼š

```{code-cell}
# -*- coding: utf-8 -*-
import os
import uuid

from langchain.agents import AgentState, create_agent
from langchain.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore

from agentscope_runtime.engine import AgentApp
from agentscope_runtime.engine.schemas.agent_schemas import AgentRequest

global_short_term_memory = None
global_long_term_memory = None

@tool
def get_weather(location: str, date: str) -> str:
    """Get the weather for a location and date."""
    print(f"Getting weather for {location} on {date}...")
    return f"The weather in {location} is sunny with a temperature of 25Â°C."

# Create the AgentApp instance
agent_app = AgentApp(
    app_name="LangGraphAgent",
    app_description="A LangGraph-based research assistant",
)

class CustomAgentState(AgentState):
    user_id: str
    session_id: dict

# Initialize services as instance variables
@agent_app.init
async def init_func(self):
    global global_short_term_memory
    global global_long_term_memory
    self.short_term_mem = InMemorySaver()
    self.long_term_mem = InMemoryStore()
    global_short_term_memory = self.short_term_mem
    global_long_term_memory = self.long_term_mem

# Query endpoint for LangGraph integration with tools
@agent_app.query(framework="langgraph")
async def query_func(
    self,
    msgs,
    request: AgentRequest = None,
    **kwargs,
):
    # Extract session information
    session_id = request.session_id
    user_id = request.user_id
    print(f"Received query from user {user_id} with session {session_id}")

    tools = [get_weather]

    # Choose the LLM that will drive the agent
    llm = ChatOpenAI(
        model="qwen-plus",
        api_key=os.environ.get("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    namespace_for_long_term_memory = (user_id, "memories")

    prompt = """You are a proactive research assistant. """

    agent = create_agent(
        llm,
        tools,
        system_prompt=prompt,
        checkpointer=self.short_term_mem,
        store=self.long_term_mem,
        state_schema=CustomAgentState,
        name="LangGraphAgent",
    )

    async for chunk, meta_data in agent.astream(
        input={"messages": msgs, "session_id": session_id, "user_id": user_id},
        stream_mode="messages",
        config={"configurable": {"thread_id": session_id}},
    ):
        is_last_chunk = (
            True if getattr(chunk, "chunk_position", "") == "last" else False
        )
        if meta_data["langgraph_node"] == "tools":
            memory_id = str(uuid.uuid4())
            memory = {"lastest_tool_call": chunk.name}
            global_long_term_memory.put(
                namespace_for_long_term_memory,
                memory_id,
                memory,
            )
        yield chunk, is_last_chunk
if __name__ == "__main__":
  agent_app.run(host="127.0.0.1", port=8090)
```

## âš™ï¸ å…ˆå†³æ¡ä»¶

```{note}
åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»å®‰è£…äº† AgentScope Runtime å¹¶é…ç½®äº†å¿…è¦çš„ API å¯†é’¥ã€‚
```

1. **å®‰è£…ä¾èµ–**ï¼š
   ```bash
   pip install agentscope-runtime
   ```

2. **è®¾ç½®ç¯å¢ƒå˜é‡**ï¼š
   ```bash
   # å¿…éœ€ï¼šDashScope API å¯†é’¥ç”¨äº Qwen æ¨¡å‹
   export DASHSCOPE_API_KEY="your-dashscope-api-key"
   ```

## â–¶ï¸ è¿è¡Œç¤ºä¾‹

```{tip}
ç¡®ä¿æ‚¨å·²ç»åœ¨å…ˆå†³æ¡ä»¶éƒ¨åˆ†è®¾ç½®äº†æ‰€æœ‰å¿…éœ€çš„ç¯å¢ƒå˜é‡ï¼Œç„¶åå†è¿è¡Œè¿™äº›ç¤ºä¾‹ã€‚
```

å¯åŠ¨æœåŠ¡å™¨åï¼Œæ‚¨å¯ä»¥é€šè¿‡æŸ¥è¯¢ç•Œé¢ä¸æ™ºèƒ½ä½“äº¤äº’ï¼Œå¹¶é€šè¿‡æä¾›çš„ API ç«¯ç‚¹æ£€æŸ¥å†…å­˜çŠ¶æ€ã€‚

### ä¸æ™ºèƒ½ä½“äº¤äº’

æœåŠ¡å™¨è¿è¡Œåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `/query` ç«¯ç‚¹å‘æ™ºèƒ½ä½“å‘é€æŸ¥è¯¢ï¼š

```bash
curl -N \
  -X POST "http://localhost:8090/process" \
  -H "Content-Type: application/json" \
  -d '{
    "input": [
      {
        "role": "user",
        "content": [
          { "type": "text", "text": "ä¸Šæµ·å¤©æ°”å¦‚ä½•ï¼Ÿ" }
        ]
      }
    ],
    "session_id": "session_id_123",
    "user_id": "user_id_123"
  }'
```

## âœ¨ å…³é”®ç‰¹æ€§å±•ç¤º

### LangGraph é›†æˆ
- ä½¿ç”¨ `AgentState` è¿›è¡ŒçŠ¶æ€ç®¡ç†
- ä½¿ç”¨ `StateGraph` å®šä¹‰å·¥ä½œæµ
- æ£€æŸ¥ç‚¹æœºåˆ¶å®ç°æŒä¹…åŒ–çŠ¶æ€
- æµå¼å“åº”å®ç°å®æ—¶äº¤äº’

### å†…å­˜ç®¡ç†
- çŸ­æœŸè®°å¿†ç”¨äºå¯¹è¯å†å²
- é•¿æœŸè®°å¿†ç”¨äºæŒä¹…åŒ–å­˜å‚¨
- API ç«¯ç‚¹æ£€æŸ¥å†…å­˜çŠ¶æ€
- åŸºäºä¼šè¯çš„å†…å­˜éš”ç¦»

### å·¥å…·é›†æˆ
- ä½¿ç”¨ LangChain çš„ `@tool` è£…é¥°å™¨å®šä¹‰è‡ªå®šä¹‰å·¥å…·
- å·¥å…·è°ƒç”¨å’Œç»“æœå¤„ç†

## ğŸŒ API ç«¯ç‚¹

```{important}
ä»¥ä¸‹ API ç«¯ç‚¹ä»…åœ¨è¿è¡Œé«˜çº§æ™ºèƒ½ä½“ç¤ºä¾‹æ—¶å¯ç”¨ã€‚
```

è¿è¡Œé«˜çº§æ™ºèƒ½ä½“ç¤ºä¾‹æ—¶ï¼Œä»¥ä¸‹ API ç«¯ç‚¹å¯ç”¨ï¼š

- `POST /process` - å‘æ™ºèƒ½ä½“å‘é€æŸ¥è¯¢
- `GET /api/memory/short-term/{session_id}` - è·å–ä¼šè¯çš„çŸ­æœŸè®°å¿†
- `GET /api/memory/short-term` - åˆ—å‡ºæ‰€æœ‰çŸ­æœŸè®°å¿†
- `GET /api/memory/long-term/{user_id}` - è·å–ç”¨æˆ·çš„é•¿æœŸè®°å¿†

## ğŸ”§ è‡ªå®šä¹‰

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è‡ªå®šä¹‰è¿™äº›ç¤ºä¾‹ï¼š

1. **æ·»åŠ æ–°å·¥å…·**ï¼šä½¿ç”¨ `@tool` è£…é¥°å™¨å®šä¹‰è‡ªå®šä¹‰å·¥å…·
2. **æ›´æ”¹ LLM**ï¼šä¿®æ”¹ `ChatOpenAI` åˆå§‹åŒ–ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
3. **æ‰©å±•å·¥ä½œæµ**ï¼šå‘çŠ¶æ€å›¾æ·»åŠ æ–°èŠ‚ç‚¹å’Œè¾¹
4. **è‡ªå®šä¹‰å†…å­˜**ï¼šå®ç°ä¸åŒçš„å†…å­˜å­˜å‚¨åç«¯

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LangGraph æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [AgentScope Runtime æ–‡æ¡£](https://runtime.agentscope.io/)